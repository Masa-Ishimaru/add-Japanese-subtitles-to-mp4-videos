{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d343129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from moviepy.editor import VideoFileClip\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import timedelta\n",
    "import subprocess\n",
    "from srt import Subtitle\n",
    "import srt\n",
    "import shutil\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6862cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なフォルダを作成\n",
    "\n",
    "os.makedirs('input_video', exist_ok = True)                   # 翻訳字幕を付けたいmp4 videoを入れる その他は空でよい\n",
    "os.makedirs('output_video_with_subtitle', exist_ok = True)\n",
    "os.makedirs('input_audio', exist_ok = True)\n",
    "os.makedirs('csv_files', exist_ok = True)\n",
    "os.makedirs('excel_files_for_srt', exist_ok = True)\n",
    "os.makedirs('excel_files_for_srt_ja', exist_ok = True)\n",
    "os.makedirs('srt_files_ja', exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c34552ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sf9m-\\OneDrive\\デスクトップ\\+mp4_video_subtitle_en2ja_DLC3\n"
     ]
    }
   ],
   "source": [
    "wp_path = os.getcwd()\n",
    "print(wp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ffc1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./input_video/(C3W1L01)_Improving Model Performance.mp4',\n",
       " './input_video/(C3W1L02 )_Orthogonalization.mp4',\n",
       " './input_video/(C3W1L03)_Single Number Evaluation Metric.mp4',\n",
       " './input_video/(C3W1L04)_Satisficing and Optimizing Metrics.mp4',\n",
       " './input_video/(C3W1L05)_Train⧸Dev⧸Test Set Distributions.mp4',\n",
       " './input_video/(C3W1L06)_Sizeof Dev and Test Sets.mp4',\n",
       " './input_video/(C3W1L07)_When to Change Dev⧸Test Sets.mp4',\n",
       " './input_video/(C3W1L08)_WhyHumanLevelPerformance.mp4',\n",
       " './input_video/(C3W1L09)_Avoidable Bias.mp4',\n",
       " './input_video/(C3W1L10)_Understanding Human-Level Performance？.mp4',\n",
       " './input_video/(C3W1L11)_Surpassing Human-Level Performance.mp4',\n",
       " './input_video/(C3W1L12)_Improving Model Performance.mp4',\n",
       " './input_video/(C3W2L01)_Carrying Out Error Analysis.mp4',\n",
       " './input_video/(C3W2L02)_Cleaning Up Incorrectly Labelled Data.mp4',\n",
       " './input_video/(C3W2L03)_Build First System Quickly, Then Iterate.mp4',\n",
       " './input_video/(C3W2L04)_Training and Testing on Different Distributions.mp4',\n",
       " './input_video/(C3W2L05)_Bias and Variance With Mismatched Data.mp4',\n",
       " './input_video/(C3W2L06)_Addressing Data Mismatch.mp4',\n",
       " './input_video/(C3W2L07)_Transfer Learning.mp4',\n",
       " './input_video/(C3W2L08)_Multitask Learning.mp4',\n",
       " './input_video/(C3W2L09)_What is end-to-end deep learning？.mp4',\n",
       " './input_video/(C3W2L10)_Whether to Use End-To-End Deep Learning.mp4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('./input_video/*.mp4')\n",
    "files = [file.replace('\\\\', '/') for file in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "504a2d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./input_video/(C3W1L01)_Improving Model Performance.mp4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e761f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathを渡すと動画名のみを返す関数\n",
    "\n",
    "def extract_video_name(video_path):\n",
    "    p = r'\\/(.*)\\.'\n",
    "    video_name = re.findall(p, video_path)\n",
    "    video_name = video_name[0].split('/')[-1]\n",
    "    return video_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9ac4a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(C3W1L01)_Improving Model Performance'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テスト\n",
    "\n",
    "extract_video_name(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296b2db",
   "metadata": {},
   "source": [
    "## mp4からmp3を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089273b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./input_audio/(C3W1L01)_Improving Model Performance.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L02 )_Orthogonalization.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L03)_Single Number Evaluation Metric.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L04)_Satisficing and Optimizing Metrics.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L05)_Train⧸Dev⧸Test Set Distributions.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L06)_Sizeof Dev and Test Sets.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L07)_When to Change Dev⧸Test Sets.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L08)_WhyHumanLevelPerformance.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L09)_Avoidable Bias.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L10)_Understanding Human-Level Performance？.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L11)_Surpassing Human-Level Performance.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W1L12)_Improving Model Performance.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W2L01)_Carrying Out Error Analysis.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W2L02)_Cleaning Up Incorrectly Labelled Data.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W2L03)_Build First System Quickly, Then Iterate.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W2L04)_Training and Testing on Different Distributions.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W2L05)_Bias and Variance With Mismatched Data.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W2L06)_Addressing Data Mismatch.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W2L07)_Transfer Learning.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W2L08)_Multitask Learning.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W2L09)_What is end-to-end deep learning？.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in ./input_audio/(C3W2L10)_Whether to Use End-To-End Deep Learning.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    input_path = file\n",
    "    output_path_mother = './input_audio/'\n",
    "    video_name = extract_video_name(file)\n",
    "    \n",
    "    # ビデオから音声ファイルを抽出しmp3で別のフォルダに保存\n",
    "    video_clip = VideoFileClip(file)\n",
    "    video_clip.audio.write_audiofile(output_path_mother + video_name + '.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04666832",
   "metadata": {},
   "source": [
    "## 文字起こし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8889377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./input_audio/(C3W1L01)_Improving Model Performance.mp3',\n",
       " './input_audio/(C3W1L02 )_Orthogonalization.mp3',\n",
       " './input_audio/(C3W1L03)_Single Number Evaluation Metric.mp3',\n",
       " './input_audio/(C3W1L04)_Satisficing and Optimizing Metrics.mp3',\n",
       " './input_audio/(C3W1L05)_Train⧸Dev⧸Test Set Distributions.mp3',\n",
       " './input_audio/(C3W1L06)_Sizeof Dev and Test Sets.mp3',\n",
       " './input_audio/(C3W1L07)_When to Change Dev⧸Test Sets.mp3',\n",
       " './input_audio/(C3W1L08)_WhyHumanLevelPerformance.mp3',\n",
       " './input_audio/(C3W1L09)_Avoidable Bias.mp3',\n",
       " './input_audio/(C3W1L10)_Understanding Human-Level Performance？.mp3',\n",
       " './input_audio/(C3W1L11)_Surpassing Human-Level Performance.mp3',\n",
       " './input_audio/(C3W1L12)_Improving Model Performance.mp3',\n",
       " './input_audio/(C3W2L01)_Carrying Out Error Analysis.mp3',\n",
       " './input_audio/(C3W2L02)_Cleaning Up Incorrectly Labelled Data.mp3',\n",
       " './input_audio/(C3W2L03)_Build First System Quickly, Then Iterate.mp3',\n",
       " './input_audio/(C3W2L04)_Training and Testing on Different Distributions.mp3',\n",
       " './input_audio/(C3W2L05)_Bias and Variance With Mismatched Data.mp3',\n",
       " './input_audio/(C3W2L06)_Addressing Data Mismatch.mp3',\n",
       " './input_audio/(C3W2L07)_Transfer Learning.mp3',\n",
       " './input_audio/(C3W2L08)_Multitask Learning.mp3',\n",
       " './input_audio/(C3W2L09)_What is end-to-end deep learning？.mp3',\n",
       " './input_audio/(C3W2L10)_Whether to Use End-To-End Deep Learning.mp3']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_files = glob.glob('./input_audio/*.mp3')\n",
    "audio_files = [file.replace('\\\\', '/') for file in audio_files]\n",
    "audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a76fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model('large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab52d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio duration =  7.02 min\n",
      "\n",
      "[00:00.000 --> 00:07.720]  We talked about how you want your learning algorithm to do well in the training set.\n",
      "[00:07.720 --> 00:10.800]  But sometimes you don't actually want to do too well.\n",
      "[00:10.800 --> 00:15.280]  And knowing what human level performance is can tell you exactly how well, but\n",
      "[00:15.280 --> 00:18.360]  not too well, you want your algorithm to do on the training set.\n",
      "[00:18.360 --> 00:19.640]  Let me show you what I mean.\n",
      "[00:19.640 --> 00:22.360]  We've used cat classification a lot.\n",
      "[00:22.360 --> 00:27.680]  And given a picture, let's say humans have near perfect accuracy.\n",
      "[00:27.680 --> 00:32.240]  So the human level error is 1%.\n",
      "[00:32.240 --> 00:39.200]  In that case, if your learning algorithm achieves 8% training error and 10% depth error,\n",
      "[00:39.200 --> 00:44.520]  then maybe you wanted to do better on the training set.\n",
      "[00:44.520 --> 00:50.280]  So the fact that there's a huge gap between how well your algorithm does on your training set\n",
      "[00:50.280 --> 00:55.680]  versus how well humans do shows that your algorithm isn't even fitting the training set well.\n",
      "[00:55.680 --> 00:57.600]  So in terms of tools,\n",
      "[00:57.600 --> 01:03.840]  to reduce bias or variance, in this case, I would say focus on reducing bias.\n",
      "[01:03.840 --> 01:09.480]  So you want to do things like train a bigger neural network or run gradient descent longer.\n",
      "[01:09.480 --> 01:12.440]  Just try to do better on the training set.\n",
      "[01:12.440 --> 01:19.160]  But now let's look at the same training error and depth error and imagine that human level performance was not 1%.\n",
      "[01:19.160 --> 01:20.720]  So let me just copy this over.\n",
      "[01:20.720 --> 01:27.360]  But in a different application or maybe on a different data set, let's say that human level error\n",
      "[01:27.360 --> 01:30.120]  is actually 7.5%.\n",
      "[01:30.120 --> 01:37.920]  Maybe the images in your data set are so blurry that even humans can't tell whether there's a cat in this picture.\n",
      "[01:37.920 --> 01:44.600]  This example is maybe slightly contrived because humans are actually very good at looking at pictures and telling if there's a cat in it or not.\n",
      "[01:44.600 --> 01:46.440]  But for the sake of this example,\n",
      "[01:46.440 --> 01:54.800]  let's say your data set's images are so blurry or so low resolution that even humans get 7.5% error.\n",
      "[01:54.800 --> 01:56.160]  In this case,\n",
      "[01:56.160 --> 01:57.240]  even though you're training\n",
      "[01:57.240 --> 02:00.320]  error and depth error are the same as the earlier example,\n",
      "[02:00.320 --> 02:03.840]  you see that maybe you're actually doing just fine on the training set.\n",
      "[02:03.840 --> 02:07.920]  It's doing only a little bit worse than human level performance.\n",
      "[02:07.920 --> 02:10.040]  And in this second example,\n",
      "[02:10.040 --> 02:14.240]  you would maybe want to focus on reducing this component,\n",
      "[02:14.240 --> 02:19.360]  reducing the variance in your learning algorithm.\n",
      "[02:19.360 --> 02:25.440]  So you might try regularization to try to bring your depth error closer to your training error, for example.\n",
      "[02:25.440 --> 02:27.040]  So in the earlier question,\n",
      "[02:27.040 --> 02:32.200]  when I was talking about the error causes discussion on bias and variance,\n",
      "[02:32.200 --> 02:38.120]  we were mainly assuming that there were tasks where Bayes error is nearly 0.\n",
      "[02:38.120 --> 02:41.200]  So to explain what just happened here,\n",
      "[02:41.200 --> 02:43.160]  for our cat classification example,\n",
      "[02:43.160 --> 02:55.640]  think of human level error as a proxy or as an estimate for Bayes' error,\n",
      "[02:55.640 --> 02:56.880]  for Bayes' optimal error.\n",
      "[02:56.880 --> 03:01.680]  computer vision tasks, this is a pretty reasonable proxy because humans are\n",
      "[03:01.680 --> 03:06.420]  actually very good at computer vision and so whatever a human can do is maybe\n",
      "[03:06.420 --> 03:11.220]  not too far from Bayes' error. By definition human level error is worse\n",
      "[03:11.220 --> 03:16.440]  than Bayes' error because nothing could be better than Bayes' error but human level\n",
      "[03:16.440 --> 03:21.720]  error might not be too far from Bayes' error. So the surprising thing we saw here\n",
      "[03:21.720 --> 03:26.700]  is that depending on what human level error is or really this is really an\n",
      "[03:26.700 --> 03:32.340]  approximately Bayes' error, or so we assume it to be, but depending on what we\n",
      "[03:32.340 --> 03:40.140]  think is achievable with the same training error and depth error in these\n",
      "[03:40.140 --> 03:45.120]  two cases we decided to focus on bias reduction tactics or on variance\n",
      "[03:45.120 --> 03:52.620]  reduction tactics. And what happened is in the example on the left, 8% training\n",
      "[03:52.620 --> 03:56.040]  error is really high when you think you could get it down to\n",
      "[03:56.040 --> 03:56.660]  1% error.\n",
      "[03:56.660 --> 03:56.680]  So we can see that the training error is really high when you think you could get it down to 1% error.\n"
     ]
    }
   ],
   "source": [
    "for audio_file in audio_files:\n",
    "    clear_output()\n",
    "    \n",
    "    # オーディオファイル情報の表示\n",
    "    sound_input = AudioSegment.from_file(audio_file, format = \"mp3\")\n",
    "    sound_duration = sound_input.duration_seconds\n",
    "    print('audio duration = ', round(sound_duration / 60, 2), 'min\\n')    \n",
    "\n",
    "    # 文字起こし\n",
    "    result  = model.transcribe(audio_file, verbose = True, language = \"en\")\n",
    "    seginfo = result[\"segments\"]\n",
    "    out_text= []\n",
    "\n",
    "    # segment情報から発言の開始/終了時間とテキストを抜き出し、srt形式で編集する\n",
    "    for data in seginfo:\n",
    "        start = data[\"start\"]\n",
    "        end   = data[\"end\"]\n",
    "        text  = data[\"text\"]\n",
    "        out_line = Subtitle(index = 1,\n",
    "                    start = timedelta(seconds = timedelta(seconds = start).seconds,\n",
    "                    microseconds = timedelta(seconds = start).microseconds),\n",
    "                    end = timedelta(seconds = timedelta(seconds = end).seconds,\n",
    "                    microseconds = timedelta(seconds = end).microseconds),\n",
    "                    content = text,\n",
    "                    proprietary = '')\n",
    "        out_text.append(out_line)\n",
    "        \n",
    "        \n",
    "    # srt形式のファイルをcsv形式に編集して保存する。\n",
    "    audio_name = extract_video_name(audio_file)\n",
    "\n",
    "    with open(\"csv_files/\" + audio_name + \".csv\", mode = \"w\", encoding = \"utf-8_sig\") as f:\n",
    "        origin = srt.compose(out_text)\n",
    "        origin = origin.replace(\",\", \".\")\n",
    "        origin = origin.replace(\"\\n\", \",\")\n",
    "        origin = origin.replace(\",,\", \"\\n\")\n",
    "        f.write(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b917115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c760d0f",
   "metadata": {},
   "source": [
    "## csv_filesフィルダ内の全ての en csvファイルをExcelファイルに変換し、excel_file_for_srtフォルダに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54ca7da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./csv_files/Joe Rogan Experience 2190 - Peter Thiel_last_part.csv']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv ファイルのパスを取得\n",
    "\n",
    "files = glob.glob('./csv_files/*.csv')\n",
    "files = [file.replace('\\\\', '/') for file in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "500b05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_name(file_path):\n",
    "    p = r'\\/(.*)\\.'\n",
    "    file_name = re.findall(p, file_path)\n",
    "    file_name = file_name[0].split('/')[-1]\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a06ebab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joe Rogan Experience 2190 - Peter Thiel_last_part'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "extract_file_name(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "767c4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvファイルを同名でexcelに変換し保存\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header = None)\n",
    "    file_name = extract_file_name(file)\n",
    "    df.to_excel('excel_files_for_srt/' + file_name + '.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2fb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e88fb26",
   "metadata": {},
   "source": [
    "# <font color=\"red\">SUSPEND!!! $\\;$ excel_files_for_srtフォルダ内のすべてのExcelファイルを、google翻訳などで変換し、excel_files_for_srt_jaフォルダに入れてください。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3514f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d97c17a2",
   "metadata": {},
   "source": [
    "## excel_files_for_srt_ja内のすべての日本語excelファイルを、srtファイルに自動で変換し、srt_files_jaフォルダにコピーする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5470177d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./excel_files_for_srt_ja/Joe Rogan Experience 2190 - Peter Thiel_last_part.xlsx']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 日本語Excelファイルのパスを取得\n",
    "\n",
    "files = glob.glob('./excel_files_for_srt_ja/*.xlsx')\n",
    "files = [file.replace('\\\\', '/') for file in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c81b047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excelファイルをsrtファイルにしやすくする変換\n",
    "\n",
    "def excel2pre_srt(df):\n",
    "    \n",
    "    df_process_0 = pd.DataFrame(df[0])\n",
    "    df_process_0 = df_process_0.rename(columns = {0:'counter'})\n",
    "    df_process_1 = df[1].str.split(' --> ', expand = True)\n",
    "    df_process_1.columns = ['start','end']\n",
    "    df_process_2 = df_process_1['start'].str.split('.', expand = True)\n",
    "    df_process_2.columns = ['start','start_milli']\n",
    "    df_process_3 = df_process_1['end'].str.split('.', expand = True)\n",
    "    df_process_3.columns = ['end','end_milli']\n",
    "    df_pre_srt = pd.concat([df_process_0, df_process_2, df_process_3, df_process[2]], axis = 1)\n",
    "    df_pre_srt = df_pre_srt.rename(columns = {2:'text'})\n",
    "    \n",
    "    return df_pre_srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5dd5b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    df_ja = pd.read_excel(file)\n",
    "    df_process = df_ja.copy()\n",
    "    \n",
    "    df_pre_srt = excel2pre_srt(df_process)\n",
    "    \n",
    "    video_name = extract_video_name(file)\n",
    "\n",
    "    with open(\"srt_files_ja/\" + video_name + \".srt\", mode = \"w\", encoding = \"utf-8\") as f:\n",
    "        for row in range(df_pre_srt.shape[0]):\n",
    "\n",
    "            counter     = df_pre_srt.loc[row, \"counter\"]\n",
    "            start       = df_pre_srt.loc[row, \"start\"].strip()\n",
    "            start_milli = df_pre_srt.loc[row, \"start_milli\"].strip()\n",
    "            end         = df_pre_srt.loc[row, \"end\"].strip()\n",
    "            end_milli   = df_pre_srt.loc[row, \"end_milli\"].strip()\n",
    "            text        = df_pre_srt.loc[row, \"text\"].strip()\n",
    "\n",
    "            print(counter, file = f)\n",
    "            print(start + ',' + start_milli + ' --> ' + end + ',' + end_milli, file = f)\n",
    "            print(text, file = f)\n",
    "            print(\"\", file = f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eff986",
   "metadata": {},
   "source": [
    "## 字幕生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8518dfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./input_video/Joe Rogan Experience 2190 - Peter Thiel_last_part.mp4']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# videoファイルのパスの所得\n",
    "\n",
    "files = glob.glob('./input_video/*.mp4')\n",
    "files = [file.replace('\\\\', '/') for file in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0daa94a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing  Joe Rogan Experience 2190 - Peter Thiel_last_part ...\n",
      "...  Joe Rogan Experience 2190 - Peter Thiel_last_part  --- Done\n",
      "\n",
      "ALL DONE!\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    input_path = file\n",
    "    video_name = extract_video_name(file)\n",
    "    print('processing ', video_name, '...')\n",
    "    \n",
    "    # assファイルの作成\n",
    "    new_path = shutil.copy('./srt_files_ja/' + video_name + '.srt', 'temp.srt')\n",
    "    cmd = \"ffmpeg -i temp.srt temp.ass\"\n",
    "    res = subprocess.call(cmd, shell = True)\n",
    "    \n",
    "    # 字幕処理するファイルを作業場にコピー\n",
    "    new_path = shutil.copy(file, 'temp_video.mp4')\n",
    "    \n",
    "    # 字幕付け\n",
    "    command = \"ffmpeg -i temp_video.mp4 -vf ass=temp.ass temp_video_sub.mp4\"\n",
    "    res = subprocess.call(command, shell = True)\n",
    "    \n",
    "    # Videoを保存\n",
    "    os.rename('temp_video_sub.mp4', video_name + '_sub.mp4')\n",
    "    new_path = shutil.move(video_name + '_sub.mp4', 'output_video_with_subtitle/' + video_name + '_sub.mp4')\n",
    "    \n",
    "    # 作業場のファイルを削除\n",
    "    os.remove('temp_video.mp4')\n",
    "    os.remove('temp.srt')\n",
    "    os.remove('temp.ass')\n",
    "    \n",
    "    print('... ', video_name, ' --- Done\\n')\n",
    "    \n",
    "print('ALL DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a28986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d86f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c59adf7",
   "metadata": {},
   "source": [
    "## 引き続き動画を圧縮する場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ebaa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmd_3 = 'ffmpeg -i Takataken_Kinugawa_sub.mp4 -crf 40 Takataken_Kinugawa_sub_small.mp4'\n",
    "# subprocess.call(cmd_3, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c992826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18f87bc1",
   "metadata": {},
   "source": [
    "## movieをカットする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8e8c4f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # 必要に応じてmp4ファイルの情報を取得\n",
    "\n",
    "# import sys\n",
    "# import ffmpeg\n",
    "# from pprint import pprint\n",
    "\n",
    "# in_filename = sys.argv[1]\n",
    "# probe = ffmpeg.probe('test.mp4')\n",
    "# for stream in probe['streams']:\n",
    "#     print('stream {0}: {1}'.format(stream['index'],stream['codec_type']))\n",
    "#     pprint(stream)\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8462b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output.mp4.\n",
      "MoviePy - Writing audio in outputTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "file_path = \"test.mp4\"\n",
    "\n",
    "start = 9740          # in second\n",
    "end = start + 2000\n",
    "\n",
    "save_path = \"output.mp4\"\n",
    "\n",
    "video = VideoFileClip(file_path).subclip(start, end) \n",
    "video.write_videofile(save_path,fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4abf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
